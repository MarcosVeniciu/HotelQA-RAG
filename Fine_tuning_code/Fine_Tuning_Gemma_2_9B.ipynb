{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/Fine_Tuning_Gemma_2_9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1itP7wo0IUx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spr03zaLNzgL"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"Gemma_2\" # Nome da pasta do projeto\n",
        "salvar_GDrive = False # Se True, salva os checkpoints no Google Drive"
      ],
      "metadata": {
        "id": "iwX42iudDgby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3y5la8xOshp"
      },
      "outputs": [],
      "source": [
        "if salvar_GDrive:\n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/gemma-2-9b-it-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"\\n{model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "AWtq73_YOkDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar tokens especiais\n",
        "special_tokens_dict = {'additional_special_tokens': ['<context>', '<end_of_context>', '<question>', '<end_of_question>', '<res>', '<end_of_res>', '<system>', '<end_of_system>']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# Verificar se os tokens foram adicionados\n",
        "print(\"Tokens Especiais Adicionados:\\n\", tokenizer.additional_special_tokens)\n",
        "\n",
        "# Expandir a matriz de embeddings para incluir os novos tokens\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "tnkr8bxrvPKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "print(f\"\\n{model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "_JfkwfTxO6CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para formatar o prompt no estilo do chat template\n",
        "def formatPrompt(example):\n",
        "    instrucao = \"You are an assistant who answers questions based on the given context. Generate an answer to a question contained in <question> based on the context in <context>. Enclose your answer between <res> and end_of_/res>.\"\n",
        "    prompt = (\n",
        "\n",
        "        f\"<bos><start_of_turn>user\\n\"\n",
        "        f\"<system>{instrucao}<end_of_system>\\n<context>{example['Context']}<end_of_context>\\n<question>{example['Question']}<end_of_question><end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n\"\n",
        "        f\"<res>{example['Answer']}<end_of_res><end_of_turn><eos>\"\n",
        "    )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V2_train_16k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_train_16k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_test_16k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V2_train_16k.parquet',\n",
        "                                              'test': 'Dataset_V2_test_16k.parquet'})\n",
        "# Aplicando a função formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "SJKjbchYPDO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = os.path.join(\"/content/drive/MyDrive/Treinamento\", project_path)\n",
        "else:\n",
        "  project_dir = project_path\n",
        "print(f\"Project diretory: {project_dir}\")\n",
        "\n",
        "num_epocas = 1\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = False, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = num_epocas,\n",
        "    per_device_train_batch_size = 6,\n",
        "    gradient_accumulation_steps = 342,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=6,\n",
        "    warmup_ratio = 0.4,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "kd1EP76nPHIl",
        "outputId": "1613c9ed-67e9-4cc0-9946-eb5033f736df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project diretory: Gemma_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # começa um novo treinamento\n",
        "  print(\"Começando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()\n",
        "\n",
        "trainer.save_model(os.path.join(project_dir, \"gemma_ft_saved\"))"
      ],
      "metadata": {
        "id": "Jl_cAK_qPUHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instrucao = \"You are an assistant who answers questions based on the given context. Generate an answer to a question contained in <question> based on the context in <context>. Enclose your answer between <res> and </res>.\"\n",
        "\n",
        "contexto = \"Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\"\n",
        "pergunta2 = \"Does Hotel Fasano have a free wi-fi available for guests?\""
      ],
      "metadata": {
        "id": "pvGNQRiWrf1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": f\"<system>{instrucao}<\\system>\\n<context>{contexto}</context>\\n<question>{pergunta}</question>\" },\n",
        "    { \"role\": \"model\", \"content\": f\"<res>isso é uma ersposta</res>\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer([tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "jNi0JYhrrPo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com Gemma 2 9B](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing#scrollTo=QmUBVEnvCDJv)"
      ],
      "metadata": {
        "id": "eJtMVq7Plo0z"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZZNBoGcrCJIJ",
        "NuTB-Yrrs_v-",
        "mefWcvMIDfZE",
        "4UHvpH10DlEC",
        "87WsQO_hNzgV"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}