{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"TinyLlama\" # Nome da pasta do projeto\n",
        "salvar_GDrive = False # Se True, salva os checkpoints no Google Drive"
      ],
      "metadata": {
        "id": "aqYDvSEnJCf2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if salvar_GDrive:\n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"\\n{model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 96, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")\n",
        "print(f\"\\n{model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "py4Qz5TH4YTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Write a response that appropriately completes the request.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>{response}</res>{eos_token}\"\n",
        "  )\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    bos_token=BOS_TOKEN,\n",
        "    instruction=f\"respond: {example['Question']}\",\n",
        "    input=f\"context: {example['Context']}\",\n",
        "    response=example['Answer'],\n",
        "    eos_token=EOS_TOKEN\n",
        "  )\n",
        "\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V2_train_16k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_train_16k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_test_16k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V2_train_16k.parquet',\n",
        "                                              'test': 'Dataset_V2_test_16k.parquet'})\n",
        "# Aplicando a função formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = os.path.join(\"/content/drive/MyDrive/Treinamento\", project_path)\n",
        "else:\n",
        "  project_dir = project_path\n",
        "print(f\"Project diretory: {project_dir}\")\n",
        "\n",
        "num_epocas = 1\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = num_epocas,\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    warmup_ratio = 0.4,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # começa um novo treinamento\n",
        "  print(\"Começando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()\n",
        "\n",
        "trainer.save_model(os.path.join(project_dir, \"gemma_ft_saved\"))"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "47208412-7914-4a5a-b1ec-03773cfa0ba7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Começando um novo treinamento:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,344 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 3\n",
            "\\        /    Total batch size = 12 | Total steps = 112\n",
            " \"-____-\"     Number of trainable parameters = 75,694,080\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [112/112 18:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.414100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.257600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.947600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.811300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.704700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.659100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coisas a serem observadas**\n",
        "\n",
        "1 teste \\\n",
        "apos eu ttreinar por 26 epocas, respondeu mais ou menos bem para a tarefa, tanto quando o modelo usado era o mesmo que foi treinado, como o qando carrega ele a partir do modelo salvo. \\\n",
        "Mas ele perdeu a capacidade de responder a outros tipos de tarefas.\n",
        "Quando eu carrego o modelo a partir do modelo_base e mesclo ele com o modelo salvo, ele da uma resposta curta e fica repetindo todo o prompt em loop, porem ele não perdeu a capacidade de resolver outras tarefas.\n",
        "\n",
        "--------------------------\n",
        "\n",
        "2 teste \\\n",
        "Para o segundo teste, vou manter o uso do checkpoint, porem vou mudar a forma como uso os tokens s e /s, para ver ser isso mouda algo.\n"
      ],
      "metadata": {
        "id": "ihDW9Ej4tvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = \"Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\"\n",
        "pergunta2 = \"Does Hotel Fasano have a free wi-fi available for guests?\"\n",
        "\n",
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Write a response that appropriately completes the request.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>\"\n",
        "  )\n",
        "}\n"
      ],
      "metadata": {
        "id": "mxJqmtQmaZ64"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "9WdTbJZ7ruv9",
        "outputId": "5f657b5f-76b3-4971-9de8-4b1f005fba86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Does Hotel Fasano have a pool available for guests?</res>\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Does Hotel Fasano have a pool available for guests?</res>\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Does Hotel Fasano have a pool available for guests?</res\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta2}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "dEkzsSDuZwVI",
        "outputId": "2a34ab3e-e415-4fae-d8c3-8f759be6cfff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a free wi-fi available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Does Hotel Fasano have a free wi-fi available for guests?</res>\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a free wi-fi available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Does Hotel Fasano have a free wi-fi available for guests?</res>\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a free wi-fi available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sez8eo5pozW1",
        "outputId": "b6f2be98-5180-4c6d-d964-6d6423042338"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "1\n",
            "2\n",
            "3\n",
            "5\n",
            "8\n",
            "</res>\n",
            "\n",
            "### Instruction:\n",
            "Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Input:\n",
            "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "</res>\n",
            "\n",
            "### Instruction:\n",
            "Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Input:\n",
            "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "</res>\n",
            "\n",
            "### Instruction:\n",
            "Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "###\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo carregando o modelo salvo\n",
        "if True:\n",
        "  from unsloth import FastLanguageModel\n",
        "  modelo, tokenizero = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/Treinamento/TinyLlama_T/gemma_ft_saved\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "  )\n",
        "  FastLanguageModel.for_inference(modelo) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "inputs = tokenizero(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizero)\n",
        "_ = modelo.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "0LNHi2XUgg3m",
        "outputId": "23c00333-83ae-400c-fa49-d56236b4f67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano has a pool available for guests, providing a space for leisure and relaxation.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo a partir do modelo salvo\n",
        "FastLanguageModel.for_inference(modelo) # Enable native 2x faster inference\n",
        "inputs = tokenizero(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizero)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "xGh8a5WurEI4",
        "outputId": "7f51e8d3-34e2-48f4-b731-ad21c621c6de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Although the hotel does not have a pool, it does offer a fitness center and a restaurant with bar for guests to relax and unwind. The hotel features a parking lot and wheelchair accessibility.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo, mesclando o modelo base com o modelo salvo\n",
        "from peft import get_peft_model, PeftConfig\n",
        "\n",
        "# Carregar o modelo base\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "modele, tokenizere = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Carregar os adaptadores LoRA\n",
        "lora_path = \"/content/drive/MyDrive/Treinamento/TinyLlama_T/gemma_ft_saved\"\n",
        "# Carregar a configuração dos adaptadores LoRA\n",
        "peft_config = PeftConfig.from_pretrained(lora_path)\n",
        "\n",
        "# Combinar o modelo base com os adaptadores LoRA\n",
        "modele = get_peft_model(modele, peft_config)\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n"
      ],
      "metadata": {
        "id": "4URhu4JEhCCp",
        "outputId": "7ebef1b0-5506-426a-eeeb-a5ede84382fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n",
        "inputs = tokenizere(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizere)\n",
        "_ = modele.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "ijgpwmEzhYG_",
        "outputId": "aff9e0e4-6e23-4e71-907c-2f696844c181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "\n",
            "### Instruction:\n",
            "respond: What is the name of the hotel?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "\n",
            "### Instruction:\n",
            "respond: What is the name of the hotel?\n",
            "\n",
            "##\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n",
        "inputs = tokenizere(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizere)\n",
        "_ = modele.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "asyn0ahSrmFg",
        "outputId": "d445dbe4-7262-4c81-ea8b-7f797c94ef96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>1</res>\n",
            "<res>2</res>\n",
            "<res>3</res>\n",
            "<res>4</res>\n",
            "<res>5</res>\n",
            "<res>6</res>\n",
            "<res>7</res>\n",
            "<res>8</res>\n",
            "<res>9</res>\n",
            "<res>10</res>\n",
            "<res>11</res>\n",
            "<res>12</res>\n",
            "<res>13</res>\n",
            "<res>14</res>\n",
            "<res>15</res>\n",
            "<res>16</res>\n",
            "<res>17</res>\n",
            "<res>18</res>\n",
            "<res>19</res>\n",
            "<res>20</res>\n",
            "<res>21</res>\n",
            "<res>22</res>\n",
            "<res>23</res>\n",
            "<res>24</res>\n",
            "<res>25</res>\n",
            "<res>26</res>\n",
            "<res>27</res>\n",
            "<res>28</res>\n",
            "<res>29</res>\n",
            "<\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}