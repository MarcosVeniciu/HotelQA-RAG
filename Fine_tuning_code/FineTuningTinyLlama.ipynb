{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "py4Qz5TH4YTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "    \"prompt_context\": (\n",
        "        \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction: # Instruction on how to perform the task\\n\"\n",
        "        \"{instruction}\\n\\n\"\n",
        "        \"### Input: # Context and question\\n\"\n",
        "        \"{input}\\n\\n\"\n",
        "        \"### Response: # Model should generate the response here \\n\"\n",
        "        \"{response}{eos_token}\"\n",
        "    ),\n",
        "    \"prompt_no_context\": (\n",
        "        \"{bos_token}Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction: # Instruction on how to perform the task\\n\"\n",
        "        \"{instruction}\\n\\n\"\n",
        "        \"### Input: # Context and question\\n\"\n",
        "        \"{input}\\n\\n\"\n",
        "        \"### Response: # Model should generate the response here \\n\"\n",
        "        \"{response}{eos_token}\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  if example['Context'] == 'no_context':\n",
        "    prompt = ALPACA_PROMPT_DICT['prompt_no_context'].format(\n",
        "      bos_token=BOS_TOKEN,\n",
        "      instruction=\"If the context is 'no context', respond with a generic answer indicating that don't know the hotel.\",\n",
        "      input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "      response=example['Answer'],\n",
        "      eos_token=EOS_TOKEN\n",
        "    )\n",
        "  else:\n",
        "    prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "      bos_token=BOS_TOKEN,\n",
        "      instruction=\"When prompted to 'respond', simply generate the answer to the question asked in 'respond' and make sure your answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "      input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "      response=example['Answer'],\n",
        "      eos_token=EOS_TOKEN\n",
        "    )\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': '/content/drive/MyDrive/Treinamento/Dataset_V2_train_16k.parquet',\n",
        "                                              'test': '/content/drive/MyDrive/Treinamento/Dataset_V2_test_16k.parquet'})\n",
        "# Aplicando a função formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_dir = \"/content/drive/MyDrive/Treinamento/TinyLlama\"\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = 7,\n",
        "    per_device_train_batch_size = 3,\n",
        "    gradient_accumulation_steps = 3,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=193,\n",
        "    warmup_ratio = 0.1,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 20,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # começa um novo treinamento\n",
        "  print(\"Começando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}