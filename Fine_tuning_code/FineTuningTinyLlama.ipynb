{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"TinyLlama\" # Nome da pasta do projeto\n",
        "salvar_GDrive = False # Se True, salva os checkpoints no Google Drive"
      ],
      "metadata": {
        "id": "aqYDvSEnJCf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if salvar_GDrive:\n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"\\n{model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 96, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")\n",
        "print(f\"\\n{model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "py4Qz5TH4YTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Only respond with exact facts from context.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>{response}</res>{eos_token}\"\n",
        "  )\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    bos_token=BOS_TOKEN,\n",
        "    instruction=f\"respond: {example['Question']}\",\n",
        "    input=f\"context: {example['Context']}\",\n",
        "    response=example['Answer'],\n",
        "    eos_token=EOS_TOKEN\n",
        "  )\n",
        "\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V3_train_9.5k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/9,5K/Dataset_V3_train_9.5k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/9,5K/Dataset_V3_test_9.5k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V3_train_9.5k.parquet',\n",
        "                                              'test': 'Dataset_V3_test_9.5k.parquet'})\n",
        "# Aplicando a função formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = os.path.join(\"/content/drive/MyDrive/Treinamento\", project_path)\n",
        "else:\n",
        "  project_dir = project_path\n",
        "print(f\"Project diretory: {project_dir}\")\n",
        "\n",
        "num_epocas = 8\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = num_epocas,\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    warmup_ratio = 0.4,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # começa um novo treinamento\n",
        "  print(\"Começando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()\n",
        "\n",
        "trainer.save_model(os.path.join(project_dir, \"gemma_ft_saved\"))"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_local = \"/content/TinyLlama/checkpoint-440\""
      ],
      "metadata": {
        "id": "XHg4ADwks2WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "modelos = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  lora_local, # YOUR MODEL YOU USED FOR TRAINING\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(lora_local)\n",
        "\n",
        "print(f\"\\nTotal de Parâmetros: {modelos.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "_A4I4VuQs4G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(lora_local)\n",
        "print(f\"\\nTotal de Parâmetros: {base_model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "vutkW7BFs9Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coisas a serem observadas**\n",
        "\n",
        "1 teste \\\n",
        "apos eu ttreinar por 26 epocas, respondeu mais ou menos bem para a tarefa, tanto quando o modelo usado era o mesmo que foi treinado, como o qando carrega ele a partir do modelo salvo. \\\n",
        "Mas ele perdeu a capacidade de responder a outros tipos de tarefas.\n",
        "Quando eu carrego o modelo a partir do modelo_base e mesclo ele com o modelo salvo, ele da uma resposta curta e fica repetindo todo o prompt em loop, porem ele não perdeu a capacidade de resolver outras tarefas.\n",
        "\n",
        "--------------------------\n",
        "\n",
        "2 teste \\\n",
        "Para o segundo teste, vou manter o uso do checkpoint, porem vou mudar a forma como uso os tokens s e /s, para ver ser isso mouda algo.\n"
      ],
      "metadata": {
        "id": "ihDW9Ej4tvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = \"Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\"\n",
        "pergunta2 = \"Does Hotel Fasano have a free wi-fi available for guests?\"\n",
        "\n",
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Write a response that appropriately completes the request.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>\"\n",
        "  )\n",
        "}\n"
      ],
      "metadata": {
        "id": "mxJqmtQmaZ64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "9WdTbJZ7ruv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta2}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "dEkzsSDuZwVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "sez8eo5pozW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo carregando o modelo salvo\n",
        "if True:\n",
        "  from unsloth import FastLanguageModel\n",
        "  modelo, tokenizero = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/Treinamento/TinyLlama_T/gemma_ft_saved\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "  )\n",
        "  FastLanguageModel.for_inference(modelo) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "inputs = tokenizero(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizero)\n",
        "_ = modelo.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "0LNHi2XUgg3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo a partir do modelo salvo\n",
        "FastLanguageModel.for_inference(modelo) # Enable native 2x faster inference\n",
        "inputs = tokenizero(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizero)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "xGh8a5WurEI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo, mesclando o modelo base com o modelo salvo\n",
        "from peft import get_peft_model, PeftConfig\n",
        "\n",
        "# Carregar o modelo base\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "modele, tokenizere = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Carregar os adaptadores LoRA\n",
        "lora_path = \"/content/drive/MyDrive/Treinamento/TinyLlama_T/gemma_ft_saved\"\n",
        "# Carregar a configuração dos adaptadores LoRA\n",
        "peft_config = PeftConfig.from_pretrained(lora_path)\n",
        "\n",
        "# Combinar o modelo base com os adaptadores LoRA\n",
        "modele = get_peft_model(modele, peft_config)\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n"
      ],
      "metadata": {
        "id": "4URhu4JEhCCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n",
        "inputs = tokenizere(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizere)\n",
        "_ = modele.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "ijgpwmEzhYG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n",
        "inputs = tokenizere(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizere)\n",
        "_ = modele.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "asyn0ahSrmFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}