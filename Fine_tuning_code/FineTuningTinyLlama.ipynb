{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"TinyLlama_3\" # Nome da pasta do projeto\n",
        "salvar_GDrive = False # Se True, salva os checkpoints no Google Drive"
      ],
      "metadata": {
        "id": "aqYDvSEnJCf2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if salvar_GDrive:\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "py4Qz5TH4YTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context. Generate the output between <res> and </res>. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction: # Instruction on how to perform the task\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input: # Context and question\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response: # Model should generate the response here \\n\"\n",
        "    \"<res>{response}</res>{eos_token}\"\n",
        "  ),\n",
        "  \"prompt_no_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction: # Instruction on how to perform the task\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input: # Context and question\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response: # Model should generate the response here \\n\"\n",
        "    \"<res>{response}</res>{eos_token}\"\n",
        "  ),\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  if example['Context'] == 'No context':\n",
        "    prompt = ALPACA_PROMPT_DICT['prompt_no_context'].format(\n",
        "      bos_token=BOS_TOKEN,\n",
        "      instruction=\"If the context is 'no context', respond with a generic answer indicating that don't know the hotel.\",\n",
        "      input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "      response=example['Answer'],\n",
        "      eos_token=EOS_TOKEN\n",
        "    )\n",
        "  else:\n",
        "    if 'Yes, ' in example['Answer']:\n",
        "      prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=BOS_TOKEN,\n",
        "        instruction=\"When prompted to 'respond', simply generate the answer to the question asked in 'respond' and make sure your answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "        response=example['Answer'],\n",
        "        eos_token=EOS_TOKEN\n",
        "      )\n",
        "    else:\n",
        "      prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=BOS_TOKEN,\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "        response=example['Answer'],\n",
        "        eos_token=EOS_TOKEN\n",
        "      )\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V2_train_16k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_train_16k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_test_16k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V2_train_16k.parquet',\n",
        "                                              'test': 'Dataset_V2_test_16k.parquet'})\n",
        "# Aplicando a função formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = os.path.join(\"/content/drive/MyDrive/Treinamento\", project_path)\n",
        "else:\n",
        "  project_dir = project_path\n",
        "print(f\"Project diretory: {project_dir}\")\n",
        "\n",
        "num_epocas = 1\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = num_epocas,\n",
        "    per_device_train_batch_size = 3,\n",
        "    gradient_accumulation_steps = 3,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    warmup_ratio = 0.3,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # começa um novo treinamento\n",
        "  print(\"Começando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if True: # Salva o LoRA adapters ao final do treinamento\n",
        "  model.save_pretrained(os.path.join(project_dir, \"lora_adapters\")) # Local saving\n",
        "  tokenizer.save_pretrained(os.path.join(project_dir, \"lora_adapters\")) # Local saving\n",
        "\n",
        "if True: # Salva o modelo completo, LoRA adapters + Model base ao final do treinamento\n",
        "  model = model.merge_and_unload()\n",
        "  model.save_pretrained(os.path.join(project_dir, \"merged_model\"))\n",
        "  tokenizer.save_pretrained(os.path.join(project_dir, \"merged_model\"))\n",
        "\n",
        "if False: # Salva o modelo no HuggingFace\n",
        "  model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "  tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "y2WZE9KddFG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coisas a serem observadas**\n",
        "\n",
        "1 teste \\\n",
        "apos eu ttreinar por 26 epocas, respondeu mais ou menos bem para a tarefa, tanto quando o modelo usado era o mesmo que foi treinado, como o qando carrega ele a partir do modelo salvo. \\\n",
        "Mas ele perdeu a capacidade de responder a outros tipos de tarefas.\n",
        "Quando eu carrego o modelo a partir do modelo_base e mesclo ele com o modelo salvo, ele da uma resposta curta e fica repetindo todo o prompt em loop, porem ele não perdeu a capacidade de resolver outras tarefas.\n",
        "\n",
        "--------------------------\n",
        "\n",
        "2 teste \\\n",
        "Para o segundo teste, vou manter o uso do checkpoint, porem vou mudar a forma como uso os tokens s e /s, para ver ser isso mouda algo.\n"
      ],
      "metadata": {
        "id": "ihDW9Ej4tvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = \"Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\""
      ],
      "metadata": {
        "id": "mxJqmtQmaZ64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {contexto}\\nrespond: {pergunta}\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "dEkzsSDuZwVI",
        "outputId": "903ee27f-2903-4083-d2f8-91c6f8aa572c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "<s> Although Hotel Fasano does not have guests, it does offer This luxury hotel, located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception. ensuring a pleasant stay for guests.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo carregando o modelo salvo\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/Treinamento/TinyLlama/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {contexto}\\nrespond: {pergunta}\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "0LNHi2XUgg3m",
        "outputId": "1372631a-2c88-4b3f-f97d-3638b3531897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "<s> Although Hotel Fasano does not have guests, it does offer This luxury hotel, located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception. ensuring a pleasant stay for guests.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo a partir do modelo salvo\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"Continue the fibonnaci sequence.\",\n",
        "        input=f\"1, 1, 2, 3, 5, 8\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "xGh8a5WurEI4",
        "outputId": "dee42b60-2946-48c5-ee09-33756c685953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input: # Context and question\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "I don't know the hotel you are looking for, please provide a hotel in Cabo Frio, Rio de Janeiro, Brazil.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo, mesclando o modelo base com o modelo salvo\n",
        "from peft import get_peft_model, PeftConfig\n",
        "\n",
        "# Carregar o modelo base\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Carregar os adaptadores LoRA\n",
        "lora_path = \"/content/drive/MyDrive/Treinamento/TinyLlama/lora_model\"\n",
        "# Carregar a configuração dos adaptadores LoRA\n",
        "peft_config = PeftConfig.from_pretrained(lora_path)\n",
        "\n",
        "# Combinar o modelo base com os adaptadores LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
      ],
      "metadata": {
        "id": "4URhu4JEhCCp",
        "outputId": "260a11da-69d2-43b0-b874-a64ed621de76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {contexto}\\nrespond: {pergunta}\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 300)"
      ],
      "metadata": {
        "id": "ijgpwmEzhYG_",
        "outputId": "b5f6321e-b32e-41f5-a93c-59988f6704b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "Yes, the hotel has a pool available for guests.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "Yes, the hotel has a pool available for guests.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, São Paulo. This lux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"Continue the fibonnaci sequence.\",\n",
        "        input=f\"1, 1, 2, 3, 5, 8\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2050)"
      ],
      "metadata": {
        "id": "asyn0ahSrmFg",
        "outputId": "50dc4050-081e-4aca-cff4-32c002ada640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input: # Context and question\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17763, 28672, 46368, 75025, 121665, 196418, 318511, 518496, 832040, 1310720, 2097152, 3374503, 5184960, 8320400, 13107200, 20971520, 33745030, 51849600, 83204000, 131072000, 209715200, 337450300, 518496000, 832040000, 1310720000, 2097152000, 3374503000, 5184960000, 8320400000, 13107200000, 20971520000, 33745030000, 51849600000, 83204000000, 131072000000, 209715200000, 337450300000, 518496000000, 832040000000, 1310720000000, 2097152000000, 3374503000000, 5184960000000, 8320400000000, 13107200000000, 20971520000000, 33745030000000, 51849600000000, 83204000000000, 131072000000000, 209715200000000, 337450300000000, 518496000000000, 832040000000000, 1310720000000000, 2097152000000000, 3374503000000000, 5184960000000000, 8320400000000000, 13107200000000000, 20971520000000000, 33745030000000000, 51849600000000000, 83204000000000000, 131072000000000000, 209715200000000000, 337450300000000000, 518496000000000000, 832040000000000000, 1310720000000000000, 2097152000000000000, 3374503000000000000, 5184960000000000000, 8320400000000000000, 13107200000000000000, 20971520000000000000, 33745030000000000000, 51849600000000000000, 83204000000000000000, 131072000000000000000, 209715200000000000000, 337450300000000000000, 518496000000000000000, 832040000000000000000, 1310720000000000000000, 2097152000000000000000, 3374503000000000000000, 5184960000000000000000, 8320400000000000000000, 13107200000000000000000, 20971520000000000000000, 33745030000000000000000, 51849600000000000000000, 83204000000000000000000, 131072000000000000000000, 209715200000000000000000, 337450300000000000000000, 518496000000000000000000, 832040000000000000000000, 1310720000000000000000000, 2097152000000000000000000, 3374503000000000000000000, 5184960000000000000000000, 8320400000000000000000000, 13107200000000000000000000, 20971520000000000000000000, "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}