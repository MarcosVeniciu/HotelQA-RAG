{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b732e42772614097a6632debe1560eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b81e60b92af040968272a5113d00e124",
              "IPY_MODEL_c165f7eda87e46b69d6501115db2032a",
              "IPY_MODEL_c63faf0d542446eea9162c727a7094f6"
            ],
            "layout": "IPY_MODEL_9d334baf54734ab78ffe5be72b6986c1"
          }
        },
        "b81e60b92af040968272a5113d00e124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f509c4225edf4e3494b3cbcb06f3e955",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_99d7cc7fe0524395a12a8bcf2596c0b7",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "c165f7eda87e46b69d6501115db2032a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f184b105e90f42b795e597f2eb3d7503",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36ba79b5fb2d475a860d2844021b9cd2",
            "value": 1
          }
        },
        "c63faf0d542446eea9162c727a7094f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a920ebdf93b04a2fa3bcaafbfcf27247",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e2bd379fd79d4dae9c306f1a02a61e0c",
            "value": "â€‡1705/0â€‡[00:21&lt;00:00,â€‡212.11â€‡examples/s]"
          }
        },
        "9d334baf54734ab78ffe5be72b6986c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f509c4225edf4e3494b3cbcb06f3e955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99d7cc7fe0524395a12a8bcf2596c0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f184b105e90f42b795e597f2eb3d7503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "36ba79b5fb2d475a860d2844021b9cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a920ebdf93b04a2fa3bcaafbfcf27247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2bd379fd79d4dae9c306f1a02a61e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP",
        "outputId": "c75c22ae-e497-4932-85ce-d422e33f8514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0",
        "outputId": "9906ddac-74d5-460b-eb71-c3ca8ddce56f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf",
        "outputId": "2a76ec10-e394-4db2-97a6-47a80bdd4e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "py4Qz5TH4YTX",
        "outputId": "674ff39b-5d76-4f52-d404-d1ab1bdf49a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.8 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction: # Instruction on how to perform the task\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input: # Context and question\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response: # Model should generate the response here \\n\"\n",
        "    \"{bos_token}{response}{eos_token}\"\n",
        "  ),\n",
        "  \"prompt_no_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction: # Instruction on how to perform the task\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input: # Context and question\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response: # Model should generate the response here \\n\"\n",
        "    \"{bos_token}{response}{eos_token}\"\n",
        "  ),\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  if example['Context'] == 'No context':\n",
        "    prompt = ALPACA_PROMPT_DICT['prompt_no_context'].format(\n",
        "      bos_token=BOS_TOKEN,\n",
        "      instruction=\"If the context is 'no context', respond with a generic answer indicating that don't know the hotel.\",\n",
        "      input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "      response=example['Answer'],\n",
        "      eos_token=EOS_TOKEN\n",
        "    )\n",
        "  else:\n",
        "    if 'Yes, ' in example['Answer']:\n",
        "      prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=BOS_TOKEN,\n",
        "        instruction=\"When prompted to 'respond', simply generate the answer to the question asked in 'respond' and make sure your answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "        response=example['Answer'],\n",
        "        eos_token=EOS_TOKEN\n",
        "      )\n",
        "    else:\n",
        "      prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=BOS_TOKEN,\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {example['Context']}\\nrespond: {example['Question']}\",\n",
        "        response=example['Answer'],\n",
        "        eos_token=EOS_TOKEN\n",
        "      )\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V2_train_16k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_train_16k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_test_16k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V2_train_16k.parquet',\n",
        "                                              'test': 'Dataset_V2_test_16k.parquet'})\n",
        "# Aplicando a funÃ§Ã£o formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = \"/content/drive/MyDrive/Treinamento/TinyLlama\"\n",
        "else:\n",
        "  project_dir = \"TinyLlama\"\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = 25,\n",
        "    per_device_train_batch_size = 3,\n",
        "    gradient_accumulation_steps = 3,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=193,\n",
        "    warmup_ratio = 0.1,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 20,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf",
        "outputId": "6bed867c-8321-46d2-b011-172a3d8c13c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b732e42772614097a6632debe1560eb7",
            "b81e60b92af040968272a5113d00e124",
            "c165f7eda87e46b69d6501115db2032a",
            "c63faf0d542446eea9162c727a7094f6",
            "9d334baf54734ab78ffe5be72b6986c1",
            "f509c4225edf4e3494b3cbcb06f3e955",
            "99d7cc7fe0524395a12a8bcf2596c0b7",
            "f184b105e90f42b795e597f2eb3d7503",
            "36ba79b5fb2d475a860d2844021b9cd2",
            "a920ebdf93b04a2fa3bcaafbfcf27247",
            "e2bd379fd79d4dae9c306f1a02a61e0c"
          ]
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b732e42772614097a6632debe1560eb7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # comeÃ§a um novo treinamento\n",
        "  print(\"ComeÃ§ando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()\n",
        "\n",
        "# Salva o LoRA adapters ao final do treinamento\n",
        "model.save_pretrained(project_dir + \"/lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(project_dir + \"/lora_model\")"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3",
        "outputId": "bc30a6b5-c70d-4280-8103-cb633a1afaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuando treinamento a partir de: /content/drive/MyDrive/Treinamento/TinyLlama/checkpoint-4536\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,705 | Num Epochs = 25\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient Accumulation steps = 3\n",
            "\\        /    Total batch size = 9 | Total steps = 4,725\n",
            " \"-____-\"     Number of trainable parameters = 25,231,360\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4725' max='4725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4725/4725 20:55, Epoch 24/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4540</td>\n",
              "      <td>0.273600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4560</td>\n",
              "      <td>0.175600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4580</td>\n",
              "      <td>0.141400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.140200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4620</td>\n",
              "      <td>0.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4640</td>\n",
              "      <td>0.135900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4660</td>\n",
              "      <td>0.133100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4680</td>\n",
              "      <td>0.132700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4720</td>\n",
              "      <td>0.130200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Treinamento/TinyLlama/lora_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Treinamento/TinyLlama/lora_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Treinamento/TinyLlama/lora_model/tokenizer.model',\n",
              " '/content/drive/MyDrive/Treinamento/TinyLlama/lora_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Treinamento/TinyLlama/lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coisas a serem observadas**\n",
        "\n",
        "1 teste \\\n",
        "apos eu ttreinar por 25 epocas, respondeu mais ou menos bem para a tarefa, tanto quando o modelo usado era o mesmo que foi treinado, como o qando carrega ele a partir do modelo salvo. \\\n",
        "Mas ele perdeu a capacidade de responder a outros tipos de tarefas.\n",
        "Quando eu carrego o modelo a partir do modelo_base e mesclo ele com o modelo salvo, ele da uma resposta curta e fica repetindo todo o prompt em loop, porem ele nÃ£o perdeu a capacidade de resolver outras tarefas.\n",
        "\n",
        "--------------------------\n",
        "\n",
        "2 teste \\\n",
        "Para o segundo teste, vou manter o uso do checkpoint, porem vou mudar a forma como uso os tokens s e /s, para ver ser isso mouda algo.\n",
        "\n",
        "\n",
        "-------------------------\n",
        "\n",
        "3 teste \\\n",
        "vou treinar o modelo por 3h e ver se isso muda algo. (suspeita Ã© que usar os checkpoint esteja atrapalhando, mas deve ser outra coisa)"
      ],
      "metadata": {
        "id": "ihDW9Ej4tvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = \"Hotel Fasano, SÃ£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\""
      ],
      "metadata": {
        "id": "mxJqmtQmaZ64"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {contexto}\\nrespond: {pergunta}\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "dEkzsSDuZwVI",
        "outputId": "903ee27f-2903-4083-d2f8-91c6f8aa572c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, SÃ£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "<s> Although Hotel Fasano does not have guests, it does offer This luxury hotel, located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception. ensuring a pleasant stay for guests.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo carregando o modelo salvo\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/Treinamento/TinyLlama/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {contexto}\\nrespond: {pergunta}\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "0LNHi2XUgg3m",
        "outputId": "1372631a-2c88-4b3f-f97d-3638b3531897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, SÃ£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "<s> Although Hotel Fasano does not have guests, it does offer This luxury hotel, located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception. ensuring a pleasant stay for guests.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo a partir do modelo salvo\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"Continue the fibonnaci sequence.\",\n",
        "        input=f\"1, 1, 2, 3, 5, 8\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "xGh8a5WurEI4",
        "outputId": "dee42b60-2946-48c5-ee09-33756c685953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input: # Context and question\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "I don't know the hotel you are looking for, please provide a hotel in Cabo Frio, Rio de Janeiro, Brazil.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo, mesclando o modelo base com o modelo salvo\n",
        "from peft import get_peft_model, PeftConfig\n",
        "\n",
        "# Carregar o modelo base\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Carregar os adaptadores LoRA\n",
        "lora_path = \"/content/drive/MyDrive/Treinamento/TinyLlama/lora_model\"\n",
        "# Carregar a configuraÃ§Ã£o dos adaptadores LoRA\n",
        "peft_config = PeftConfig.from_pretrained(lora_path)\n",
        "\n",
        "# Combinar o modelo base com os adaptadores LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
      ],
      "metadata": {
        "id": "4URhu4JEhCCp",
        "outputId": "260a11da-69d2-43b0-b874-a64ed621de76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\",\n",
        "        input=f\"context: {contexto}\\nrespond: {pergunta}\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 300)"
      ],
      "metadata": {
        "id": "ijgpwmEzhYG_",
        "outputId": "b5f6321e-b32e-41f5-a93c-59988f6704b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, SÃ£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "Yes, the hotel has a pool available for guests.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, SÃ£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "Yes, the hotel has a pool available for guests.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "When prompted to 'respond', use the context to answer the question. If the hotel lacks a requested amenity, mention this and highlight other available amenities. Ensure the answer is detailed, informative and engaging, using the information in context to enhance your answer.\n",
            "\n",
            "### Input: # Context and question\n",
            "context: Hotel Fasano, SÃ£o Paulo. This lux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "        bos_token=\"\",\n",
        "        instruction=\"Continue the fibonnaci sequence.\",\n",
        "        input=f\"1, 1, 2, 3, 5, 8\",\n",
        "        response=\"\",\n",
        "        eos_token=\"\"\n",
        "      )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2050)"
      ],
      "metadata": {
        "id": "asyn0ahSrmFg",
        "outputId": "50dc4050-081e-4aca-cff4-32c002ada640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction: # Instruction on how to perform the task\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input: # Context and question\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response: # Model should generate the response here \n",
            "1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17763, 28672, 46368, 75025, 121665, 196418, 318511, 518496, 832040, 1310720, 2097152, 3374503, 5184960, 8320400, 13107200, 20971520, 33745030, 51849600, 83204000, 131072000, 209715200, 337450300, 518496000, 832040000, 1310720000, 2097152000, 3374503000, 5184960000, 8320400000, 13107200000, 20971520000, 33745030000, 51849600000, 83204000000, 131072000000, 209715200000, 337450300000, 518496000000, 832040000000, 1310720000000, 2097152000000, 3374503000000, 5184960000000, 8320400000000, 13107200000000, 20971520000000, 33745030000000, 51849600000000, 83204000000000, 131072000000000, 209715200000000, 337450300000000, 518496000000000, 832040000000000, 1310720000000000, 2097152000000000, 3374503000000000, 5184960000000000, 8320400000000000, 13107200000000000, 20971520000000000, 33745030000000000, 51849600000000000, 83204000000000000, 131072000000000000, 209715200000000000, 337450300000000000, 518496000000000000, 832040000000000000, 1310720000000000000, 2097152000000000000, 3374503000000000000, 5184960000000000000, 8320400000000000000, 13107200000000000000, 20971520000000000000, 33745030000000000000, 51849600000000000000, 83204000000000000000, 131072000000000000000, 209715200000000000000, 337450300000000000000, 518496000000000000000, 832040000000000000000, 1310720000000000000000, 2097152000000000000000, 3374503000000000000000, 5184960000000000000000, 8320400000000000000000, 13107200000000000000000, 20971520000000000000000, 33745030000000000000000, 51849600000000000000000, 83204000000000000000000, 131072000000000000000000, 209715200000000000000000, 337450300000000000000000, 518496000000000000000000, 832040000000000000000000, 1310720000000000000000000, 2097152000000000000000000, 3374503000000000000000000, 5184960000000000000000000, 8320400000000000000000000, 13107200000000000000000000, 20971520000000000000000000, "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}