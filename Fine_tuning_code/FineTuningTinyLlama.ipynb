{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"TinyLlama\" # Nome da pasta do projeto\n",
        "salvar_GDrive = False # Se True, salva os checkpoints no Google Drive"
      ],
      "metadata": {
        "id": "aqYDvSEnJCf2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if salvar_GDrive:\n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal de parâmetros: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")\n",
        "print(f\"\\nTotal de parâmetros com LoRa: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "oMl8TnVNuFru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Only respond with exact facts from Input.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>{response}</res>{eos_token}\"\n",
        "  )\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    bos_token=BOS_TOKEN,\n",
        "    instruction=f\"respond: {example['Question']}\",\n",
        "    input=example['Context']\",\n",
        "    response=example['Answer'],\n",
        "    eos_token=EOS_TOKEN\n",
        "  )\n",
        "\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V3_train_9.5k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/9,5K/Dataset_V3_train_9.5k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/9,5K/Dataset_V3_test_9.5k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V3_train_9.5k.parquet',\n",
        "                                              'test': 'Dataset_V3_test_9.5k.parquet'})\n",
        "# Aplicando a função formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = os.path.join(\"/content/drive/MyDrive/Treinamento\", project_path)\n",
        "else:\n",
        "  project_dir = project_path\n",
        "print(f\"Project diretory: {project_dir}\")\n",
        "\n",
        "num_epocas = 8\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = num_epocas,\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    warmup_ratio = 0.4,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # começa um novo treinamento\n",
        "  print(\"Começando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()\n",
        "\n",
        "trainer.save_model(os.path.join(project_dir, \"tinyllama_saved\"))"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffc613ef-b80d-4c00-e916-63652bff5546"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [440/440 1:28:50, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.354700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.308600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.254200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.189200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.138900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.069800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.997800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.925300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.843200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.769900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.732300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.683200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.444600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.412000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.339200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.328100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.305600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.296000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.285000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.274900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.273900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.269900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.274000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.260500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.263600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.255500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.265800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.259700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.262700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.263200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.262100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.256100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = \"Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\"\n",
        "pergunta2 = \"Does Hotel Fasano have a free wi-fi available for guests?\"\n",
        "\n",
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Only respond with exact facts from Input.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>\"\n",
        "  )\n",
        "}\n"
      ],
      "metadata": {
        "id": "mxJqmtQmaZ64"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"{contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "9WdTbJZ7ruv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca71897-52f6-40df-f63a-96dac99d80cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano has a pool available for guests, providing an opportunity to relax and enjoy the beautiful surroundings.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta2}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "dEkzsSDuZwVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7515e0f7-2842-4ab5-a40f-639e1f43b119"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a free wi-fi available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano offers This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views ensuring that guests can enjoy a comfortable and convenient stay.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "sez8eo5pozW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd79cc6-64e3-408b-c109-e232410ddc93"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Although the sequence is not complete, you can continue the sequence by adding 4 to the sequence. This will result in a total of 15, which can be found below.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando o modelo a partir do modelo salvo"
      ],
      "metadata": {
        "id": "rr-O8H_FJzg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo carregando o modelo salvo\n",
        "from unsloth import FastLanguageModel\n",
        "if True:\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/TinyLlama/tinyllama_saved\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "  )\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "print(f\"\\nTotal de parâmetros com LoRa: {model.num_parameters():,}\\n\\n\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "0LNHi2XUgg3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f894dd1e-dfdf-4835-b380-f1c8f31c008d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "Total de parâmetros com LoRa: 1,150,511,104\n",
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Only respond with exact facts from context.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, São Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano, São Paulo offers This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views ensuring that guests can enjoy a comfortable and convenient stay.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo a partir do modelo salvo\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "xGh8a5WurEI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69fa6d09-0f9c-4146-80f7-43779d5484af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Only respond with exact facts from context.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Although the sequence is not necessarily a fibonnaci sequence, it is a good example of a sequence that can be used to calculate the sum of the first n natural numbers. The sequence starts with 1, which is the first number in the sequence, and continues with each subsequent number, which is the sum of the previous two numbers.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}