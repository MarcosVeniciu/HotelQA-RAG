{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/HotelQA-RAG/blob/main/Fine_tuning_code/FineTuningTinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "leB4DPM5376p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "5r0dQ_5A6oWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a159ab24-375d-4cce-83a8-7b8d2f7bed62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"TinyLlama\" # Nome da pasta do projeto\n",
        "salvar_GDrive = False # Se True, salva os checkpoints no Google Drive"
      ],
      "metadata": {
        "id": "aqYDvSEnJCf2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if salvar_GDrive:\n",
        "  # Mount Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_hQOo3HplKN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b770d5-b85c-44fd-8042-86e248e9d520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"TinyLlama/TinyLlama_v1.1\",#\"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "def printParameters(model):\n",
        "  trainable_param = 0\n",
        "  total_params = 0\n",
        "  for name , param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_param += param.numel()\n",
        "  print(f\"Total params : {total_params} , trainable_params : {trainable_param} , trainable % : {100 * trainable_param / total_params} \")\n",
        "\n",
        "printParameters(model)"
      ],
      "metadata": {
        "id": "oYcCaLXY4Hpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b18a04c-765d-4486-caa4-534e9a557ab9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TinyLlama/TinyLlama_v1.1 does not have a padding token! Will use pad_token = <unk>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params : 1100048384 , trainable_params : 1034512384 , trainable % : 94.04244386399644 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,\n",
        "  r = 96, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0, # Currently only supports dropout = 0\n",
        "  bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "  use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "  random_state = 3407,\n",
        "  use_rslora = False,  # We support rank stabilized LoRA\n",
        "  loftq_config = None, # And LoftQ\n",
        ")\n",
        "printParameters(model)"
      ],
      "metadata": {
        "id": "py4Qz5TH4YTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae936e1b-d1de-425b-9ce5-67f7fdb80a13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.8 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params : 1175742464 , trainable_params : 75694080 , trainable % : 6.4379813026809245 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"{bos_token}Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Write a response that appropriately completes the request.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>{response}</res>{eos_token}\"\n",
        "  )\n",
        "}\n",
        "\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatPrompt(example):\n",
        "  prompt = ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    bos_token=BOS_TOKEN,\n",
        "    instruction=f\"respond: {example['Question']}\",\n",
        "    input=f\"context: {example['Context']}\",\n",
        "    response=example['Answer'],\n",
        "    eos_token=EOS_TOKEN\n",
        "  )\n",
        "\n",
        "  return {\"text\": prompt}\n",
        "\n",
        "\n",
        "# downlad dataset\n",
        "if not os.path.exists('Dataset_V2_train_16k.parquet'):\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_train_16k.parquet\n",
        "  !wget https://github.com/MarcosVeniciu/HotelQA-RAG/raw/main/Dataset/16k/Dataset_V2_test_16k.parquet\n",
        "\n",
        "dataset = load_dataset('parquet', data_files={'train': 'Dataset_V2_train_16k.parquet',\n",
        "                                              'test': 'Dataset_V2_test_16k.parquet'})\n",
        "# Aplicando a fun√ß√£o formatPrompt ao dataset\n",
        "dataset = dataset.map(formatPrompt, remove_columns=['Context', 'Question', 'Answer'])"
      ],
      "metadata": {
        "id": "8Pf0-rWF4n6V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  project_dir = os.path.join(\"/content/drive/MyDrive/Treinamento\", project_path)\n",
        "else:\n",
        "  project_dir = project_path\n",
        "print(f\"Project diretory: {project_dir}\")\n",
        "\n",
        "num_epocas = 5\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  tokenizer = tokenizer,\n",
        "  train_dataset = dataset['train'].shuffle(),\n",
        "  dataset_text_field = \"text\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dataset_num_proc = 2,\n",
        "  packing = True, # Packs short sequences together to save time!\n",
        "  args = TrainingArguments(\n",
        "    num_train_epochs = num_epocas,\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 3,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    warmup_ratio = 0.4,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    seed = 3407,\n",
        "    output_dir=project_dir,\n",
        "  ),\n",
        ")"
      ],
      "metadata": {
        "id": "K16Abad67idf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc98df29-66a7-4658-ff7e-26e07ca081ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project diretory: TinyLlama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#se tiver algum checkpoint salvo na pasta do projeto, ele vai continuar a partir do ultimo salvo.\n",
        "last_checkpoint = get_last_checkpoint(project_dir)\n",
        "if last_checkpoint != None: # Continua a partir do ultimo checkpoint salvo\n",
        "  print(f\"Continuando treinamento a partir de: {last_checkpoint}\\n\")\n",
        "  trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else: # come√ßa um novo treinamento\n",
        "  print(\"Come√ßando um novo treinamento:\")\n",
        "  trainer_stats = trainer.train()\n",
        "\n",
        "trainer.save_model(os.path.join(project_dir, \"gemma_ft_saved\"))"
      ],
      "metadata": {
        "id": "tEUFAShT_8W3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61bd7c48-589b-4784-ef77-be5d0c3ab2f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuando treinamento a partir de: TinyLlama/checkpoint-224\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,344 | Num Epochs = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 3\n",
            "\\        /    Total batch size = 12 | Total steps = 560\n",
            " \"-____-\"     Number of trainable parameters = 75,694,080\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [560/560 45:27, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.636500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.544600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.501400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.435700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.416400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.383600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.371100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.370200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.380200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.383300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.400500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.411600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.417400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.427700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.422300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.421300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.426400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.416700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.420600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.421700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.421600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.422800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.422400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.431300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coisas a serem observadas**\n",
        "\n",
        "1 teste \\\n",
        "apos eu ttreinar por 26 epocas, respondeu mais ou menos bem para a tarefa, tanto quando o modelo usado era o mesmo que foi treinado, como o qando carrega ele a partir do modelo salvo. \\\n",
        "Mas ele perdeu a capacidade de responder a outros tipos de tarefas.\n",
        "Quando eu carrego o modelo a partir do modelo_base e mesclo ele com o modelo salvo, ele da uma resposta curta e fica repetindo todo o prompt em loop, porem ele n√£o perdeu a capacidade de resolver outras tarefas.\n",
        "\n",
        "--------------------------\n",
        "\n",
        "2 teste \\\n",
        "Para o segundo teste, vou manter o uso do checkpoint, porem vou mudar a forma como uso os tokens s e /s, para ver ser isso mouda algo.\n"
      ],
      "metadata": {
        "id": "ihDW9Ej4tvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = \"Hotel Fasano, S√£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\"\n",
        "pergunta = \"Does Hotel Fasano have a pool available for guests?\"\n",
        "pergunta2 = \"Does Hotel Fasano have a free wi-fi available for guests?\"\n",
        "\n",
        "ALPACA_PROMPT_DICT = {\n",
        "  \"prompt_context\": (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
        "    \"Write a response that appropriately completes the request.\"\n",
        "    \"Write a response between <res> and </res>.\\n\\n\"\n",
        "    \"### Instruction:\\n\"\n",
        "    \"{instruction}\\n\\n\"\n",
        "    \"### Input:\\n\"\n",
        "    \"{input}\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"<res>\"\n",
        "  )\n",
        "}\n"
      ],
      "metadata": {
        "id": "mxJqmtQmaZ64"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "9WdTbJZ7ruv9",
        "outputId": "8a2e821a-f248-4756-f2da-ca31f596a69e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, S√£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano offers a pool available for guests to enjoy. This pool is located in the hotel's pool area and offers a relaxing space to enjoy the sunshine and take a refreshing dip.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta2}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "dEkzsSDuZwVI",
        "outputId": "c9dd2837-5c15-4e94-bc16-3431563f0582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a free wi-fi available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, S√£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano offers free Wi-Fi for guests, providing a convenient and easy way to stay connected while enjoying their stay.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sez8eo5pozW1",
        "outputId": "a87f986b-afd8-4752-d46c-c1e786226038"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Write a response between <res> and </res\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo carregando o modelo salvo\n",
        "if True:\n",
        "  from unsloth import FastLanguageModel\n",
        "  modelo, tokenizero = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/Treinamento/TinyLlama_T/gemma_ft_saved\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "  )\n",
        "  FastLanguageModel.for_inference(modelo) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "inputs = tokenizero(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizero)\n",
        "_ = modelo.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "0LNHi2XUgg3m",
        "outputId": "23c00333-83ae-400c-fa49-d56236b4f67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, S√£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>Yes, Hotel Fasano has a pool available for guests, providing a space for leisure and relaxation.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo a partir do modelo salvo\n",
        "FastLanguageModel.for_inference(modelo) # Enable native 2x faster inference\n",
        "inputs = tokenizero(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizero)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "id": "xGh8a5WurEI4",
        "outputId": "7f51e8d3-34e2-48f4-b731-ad21c621c6de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>Although the hotel does not have a pool, it does offer a fitness center and a restaurant with bar for guests to relax and unwind. The hotel features a parking lot and wheelchair accessibility.</res></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo, mesclando o modelo base com o modelo salvo\n",
        "from peft import get_peft_model, PeftConfig\n",
        "\n",
        "# Carregar o modelo base\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "modele, tokenizere = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Carregar os adaptadores LoRA\n",
        "lora_path = \"/content/drive/MyDrive/Treinamento/TinyLlama_T/gemma_ft_saved\"\n",
        "# Carregar a configura√ß√£o dos adaptadores LoRA\n",
        "peft_config = PeftConfig.from_pretrained(lora_path)\n",
        "\n",
        "# Combinar o modelo base com os adaptadores LoRA\n",
        "modele = get_peft_model(modele, peft_config)\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n"
      ],
      "metadata": {
        "id": "4URhu4JEhCCp",
        "outputId": "7ebef1b0-5506-426a-eeeb-a5ede84382fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n",
        "inputs = tokenizere(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "  instruction=f\"respond: {pergunta}\",\n",
        "  input=f\"context: {contexto}\")\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizere)\n",
        "_ = modele.generate(**inputs, streamer = text_streamer, max_new_tokens = 120)"
      ],
      "metadata": {
        "id": "ijgpwmEzhYG_",
        "outputId": "aff9e0e4-6e23-4e71-907c-2f696844c181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "respond: Does Hotel Fasano have a pool available for guests?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, S√£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "\n",
            "### Instruction:\n",
            "respond: What is the name of the hotel?\n",
            "\n",
            "### Input:\n",
            "context: Hotel Fasano, S√£o Paulo. This luxury hotel, example located in the Jardins neighborhood, offers elegant accommodations with stunning city views. The hotel features a pool, spa and wellness center, gourmet restaurant, fitness center, and 24-hour reception.\n",
            "\n",
            "### Response:\n",
            "<res>\n",
            "\n",
            "### Instruction:\n",
            "respond: What is the name of the hotel?\n",
            "\n",
            "##\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia com o modelo apos o treinamento\n",
        "FastLanguageModel.for_inference(modele) # Enable native 2x faster inference\n",
        "inputs = tokenizere(\n",
        "[\n",
        "  ALPACA_PROMPT_DICT['prompt_context'].format(\n",
        "    instruction=\"Continue the fibonnaci sequence.\",\n",
        "    input=f\"1, 1, 2, 3, 5, 8\"    )\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizere)\n",
        "_ = modele.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "asyn0ahSrmFg",
        "outputId": "d445dbe4-7262-4c81-ea8b-7f797c94ef96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Write a response between <res> and </res>.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "<res>1</res>\n",
            "<res>2</res>\n",
            "<res>3</res>\n",
            "<res>4</res>\n",
            "<res>5</res>\n",
            "<res>6</res>\n",
            "<res>7</res>\n",
            "<res>8</res>\n",
            "<res>9</res>\n",
            "<res>10</res>\n",
            "<res>11</res>\n",
            "<res>12</res>\n",
            "<res>13</res>\n",
            "<res>14</res>\n",
            "<res>15</res>\n",
            "<res>16</res>\n",
            "<res>17</res>\n",
            "<res>18</res>\n",
            "<res>19</res>\n",
            "<res>20</res>\n",
            "<res>21</res>\n",
            "<res>22</res>\n",
            "<res>23</res>\n",
            "<res>24</res>\n",
            "<res>25</res>\n",
            "<res>26</res>\n",
            "<res>27</res>\n",
            "<res>28</res>\n",
            "<res>29</res>\n",
            "<\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERENCIAS**\n",
        "\n",
        "[1] [Colab original com tinyLlama](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing#scrollTo=95_Nn-89DhsL)"
      ],
      "metadata": {
        "id": "782si-T7BEsJ"
      }
    }
  ]
}